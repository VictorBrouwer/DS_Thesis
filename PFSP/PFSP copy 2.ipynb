{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alns import ALNS\n",
    "from alns.accept import SimulatedAnnealing\n",
    "from alns.select import AlphaUCB\n",
    "from alns.stop import MaxIterations, MaxRuntime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 2345\n",
    "rnd.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    n_jobs: int\n",
    "    n_machines: int\n",
    "    bkv: int  # best known value\n",
    "    processing_times: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path):\n",
    "        with open(path, \"r\") as fi:\n",
    "            lines = fi.readlines()\n",
    "\n",
    "            n_jobs, n_machines, _, bkv, _ = [\n",
    "                int(num) for num in lines[1].split()\n",
    "            ]\n",
    "            processing_times = np.genfromtxt(lines[3:], dtype=int)\n",
    "\n",
    "            return cls(n_jobs, n_machines, bkv, processing_times)\n",
    "\n",
    "\n",
    "def compute_completion_times(schedule):\n",
    "    \"\"\"\n",
    "    Compute the completion time for each job of the passed-in schedule.\n",
    "    \"\"\"\n",
    "    completion = np.zeros(DATA.processing_times.shape, dtype=int)\n",
    "\n",
    "    for idx, job in enumerate(schedule):\n",
    "        for machine in range(DATA.n_machines):\n",
    "            prev_job = completion[machine, schedule[idx - 1]] if idx > 0 else 0\n",
    "            prev_machine = completion[machine - 1, job] if machine > 0 else 0\n",
    "            processing = DATA.processing_times[machine, job]\n",
    "\n",
    "            completion[machine, job] = max(prev_job, prev_machine) + processing\n",
    "\n",
    "    return completion\n",
    "\n",
    "\n",
    "def compute_makespan(schedule):\n",
    "    \"\"\"\n",
    "    Returns the makespan, i.e., the maximum completion time.\n",
    "    \"\"\"\n",
    "    return compute_completion_times(schedule)[-1, schedule[-1]]\n",
    "\n",
    "\n",
    "def plot(schedule, name):\n",
    "    \"\"\"\n",
    "    Plots a Gantt chart of the schedule for the permutation flow shop problem.\n",
    "    \"\"\"\n",
    "    n_machines, n_jobs = DATA.processing_times.shape\n",
    "\n",
    "    completion = compute_completion_times(schedule)\n",
    "    start = completion - DATA.processing_times\n",
    "\n",
    "    # Plot each job using its start and completion time\n",
    "    cmap = plt.colormaps[\"rainbow\"].resampled(n_jobs)\n",
    "    machines, length, start_job, job_colors = zip(\n",
    "        *[\n",
    "            (i, DATA.processing_times[i, j], start[i, j], cmap(j - 1))\n",
    "            for i in range(n_machines)\n",
    "            for j in range(n_jobs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots(1, figsize=(12, 6))\n",
    "    ax.barh(machines, length, left=start_job, color=job_colors)\n",
    "\n",
    "    ax.set_title(f\"{name}\\n Makespan: {compute_makespan(schedule)}\")\n",
    "    ax.set_ylabel(f\"Machine\")\n",
    "    ax.set_xlabel(f\"Completion time\")\n",
    "    ax.set_yticks(range(DATA.n_machines))\n",
    "    ax.set_yticklabels(range(1, DATA.n_machines + 1))\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(\n",
    "        self, schedule: List[int], unassigned: Optional[List[int]] = None\n",
    "    ):\n",
    "        self.schedule = schedule\n",
    "        self.unassigned = unassigned if unassigned is not None else []\n",
    "\n",
    "    def objective(self):\n",
    "        return compute_makespan(self.schedule)\n",
    "\n",
    "    def insert(self, job: int, idx: int):\n",
    "        self.schedule.insert(idx, job)\n",
    "\n",
    "    def opt_insert(self, job: int):\n",
    "        \"\"\"\n",
    "        Optimally insert the job in the current schedule.\n",
    "        \"\"\"\n",
    "        idcs_costs = all_insert_cost(self.schedule, job)\n",
    "        idx, _ = min(idcs_costs, key=lambda idx_cost: idx_cost[1])\n",
    "        self.insert(job, idx)\n",
    "\n",
    "    def remove(self, job: int):\n",
    "        self.schedule.remove(job)\n",
    "\n",
    "\n",
    "def all_insert_cost(schedule: List[int], job: int) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Computes all partial makespans when inserting a job in the schedule.\n",
    "    O(nm) using Taillard's acceleration. Returns a list of tuples of the\n",
    "    insertion index and the resulting makespan.\n",
    "\n",
    "    [1] Taillard, E. (1990). Some efficient heuristic methods for the\n",
    "    flow shop sequencing problem. European Journal of Operational Research,\n",
    "    47(1), 65-74.\n",
    "    \"\"\"\n",
    "    k = len(schedule) + 1\n",
    "    m = DATA.processing_times.shape[0]\n",
    "    p = DATA.processing_times\n",
    "\n",
    "    # Earliest completion of schedule[j] on machine i before insertion\n",
    "    e = np.zeros((m + 1, k))\n",
    "    for j in range(k - 1):\n",
    "        for i in range(m):\n",
    "            e[i, j] = max(e[i, j - 1], e[i - 1, j]) + p[i, schedule[j]]\n",
    "\n",
    "    # Duration between starting time and final makespan\n",
    "    q = np.zeros((m + 1, k))\n",
    "    for j in range(k - 2, -1, -1):\n",
    "        for i in range(m - 1, -1, -1):\n",
    "            q[i, j] = max(q[i + 1, j], q[i, j + 1]) + p[i, schedule[j]]\n",
    "\n",
    "    # Earliest relative completion time\n",
    "    f = np.zeros((m + 1, k))\n",
    "    for l in range(k):\n",
    "        for i in range(m):\n",
    "            f[i, l] = max(f[i - 1, l], e[i, l - 1]) + p[i, job]\n",
    "\n",
    "    # Partial makespan; drop the last (dummy) row of q\n",
    "    M = np.max(f + q, axis=0)\n",
    "\n",
    "    return [(idx, M[idx]) for idx in np.argsort(M)]\n",
    "\n",
    "\n",
    "def random_removal(state: Solution, rng, n_remove=2) -> Solution:\n",
    "    \"\"\"\n",
    "    Randomly remove a number jobs from the solution.\n",
    "    \"\"\"\n",
    "    destroyed = deepcopy(state)\n",
    "\n",
    "    for job in rng.choice(DATA.n_jobs, n_remove, replace=False):\n",
    "        destroyed.unassigned.append(job)\n",
    "        destroyed.schedule.remove(job)\n",
    "\n",
    "    return destroyed\n",
    "\n",
    "\n",
    "def adjacent_removal(state: Solution, rng, n_remove=2) -> Solution:\n",
    "    \"\"\"\n",
    "    Randomly remove a number adjacent jobs from the solution.\n",
    "    \"\"\"\n",
    "    destroyed = deepcopy(state)\n",
    "\n",
    "    start = rng.integers(DATA.n_jobs - n_remove)\n",
    "    jobs_to_remove = [state.schedule[start + idx] for idx in range(n_remove)]\n",
    "\n",
    "    for job in jobs_to_remove:\n",
    "        destroyed.unassigned.append(job)\n",
    "        destroyed.schedule.remove(job)\n",
    "\n",
    "    return destroyed\n",
    "\n",
    "\n",
    "def greedy_repair(state: Solution, rng, **kwargs) -> Solution:\n",
    "    \"\"\"\n",
    "    Greedily insert the unassigned jobs back into the schedule. The jobs are\n",
    "    inserted in non-decreasing order of total processing times.\n",
    "    \"\"\"\n",
    "    state.unassigned.sort(key=lambda j: sum(DATA.processing_times[:, j]))\n",
    "\n",
    "    while len(state.unassigned) != 0:\n",
    "        job = state.unassigned.pop()  # largest total processing time first\n",
    "        state.opt_insert(job)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def local_search(solution: Solution, **kwargs):\n",
    "    \"\"\"\n",
    "    Improves the current solution in-place using the insertion neighborhood.\n",
    "    A random job is selected and put in the best new position. This continues\n",
    "    until relocating any of the jobs does not lead to an improving move.\n",
    "    \"\"\"\n",
    "    improved = True\n",
    "\n",
    "    while improved:\n",
    "        improved = False\n",
    "        current = solution.objective()\n",
    "\n",
    "        for job in rnd.choice(\n",
    "            solution.schedule, len(solution.schedule), replace=False\n",
    "        ):\n",
    "            solution.remove(job)\n",
    "            solution.opt_insert(job)\n",
    "\n",
    "            if solution.objective() < current:\n",
    "                improved = True\n",
    "                current = solution.objective()\n",
    "                break\n",
    "\n",
    "\n",
    "def greedy_repair_then_local_search(state: Solution, rng, **kwargs):\n",
    "    \"\"\"\n",
    "    Greedily insert the unassigned jobs back into the schedule (using NEH\n",
    "    ordering). Apply local search afterwards.\n",
    "    \"\"\"\n",
    "    state = greedy_repair(state, rng, **kwargs)\n",
    "    local_search(state, **kwargs)\n",
    "    return state\n",
    "\n",
    "\n",
    "def NEH(processing_times: np.ndarray) -> Solution:\n",
    "    \"\"\"\n",
    "    Schedules jobs in decreasing order of the total processing times.\n",
    "\n",
    "    [1] Nawaz, M., Enscore Jr, E. E., & Ham, I. (1983). A heuristic algorithm\n",
    "    for the m-machine, n-job flow-shop sequencing problem. Omega, 11(1), 91-95.\n",
    "    \"\"\"\n",
    "    largest_first = np.argsort(processing_times.sum(axis=0)).tolist()[::-1]\n",
    "    solution = Solution([largest_first.pop(0)], [])\n",
    "\n",
    "    for job in largest_first:\n",
    "        solution.opt_insert(job)\n",
    "\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_repair(state: Solution, rng, **kwargs) -> Solution:\n",
    "    \"\"\"\n",
    "    Diverse and high-quality repair operators for PFSP using ALNS.\n",
    "    Inserts unassigned jobs back into the solution in a smart way to minimize makespan.\n",
    "    \"\"\"\n",
    "    # Copy state to avoid in-place modification if needed\n",
    "    # state = deepcopy(state)  # Uncomment if you want to avoid in-place changes\n",
    "\n",
    "    # Choose a repair strategy at random (or cycle through them in ALNS)\n",
    "    strategies = [\n",
    "        \"greedy_insertion\",\n",
    "        \"random_insertion\",\n",
    "        \"regret_insertion\",\n",
    "        \"randomized_greedy\",\n",
    "        \"block_insertion\"\n",
    "    ]\n",
    "    strategy = rng.choice(strategies)\n",
    "\n",
    "    if strategy == \"greedy_insertion\":\n",
    "        # Insert each unassigned job at the position that minimizes makespan\n",
    "        for job in state.unassigned[:]:\n",
    "            best_pos = None\n",
    "            best_makespan = float('inf')\n",
    "            for idx in range(len(state.schedule) + 1):\n",
    "                state.insert(job, idx)\n",
    "                ms = state.objective()\n",
    "                if ms < best_makespan:\n",
    "                    best_makespan = ms\n",
    "                    best_pos = idx\n",
    "                state.remove(job)\n",
    "            state.insert(job, best_pos)\n",
    "            state.unassigned.remove(job)\n",
    "\n",
    "    elif strategy == \"random_insertion\":\n",
    "        # Insert each unassigned job at a random position\n",
    "        for job in state.unassigned[:]:\n",
    "            idx = rng.integers(0, len(state.schedule) + 1)\n",
    "            state.insert(job, idx)\n",
    "            state.unassigned.remove(job)\n",
    "\n",
    "    elif strategy == \"regret_insertion\":\n",
    "        # Regret-2 insertion: insert the job with the largest regret value\n",
    "        while state.unassigned:\n",
    "            best_job = None\n",
    "            best_regret = -1\n",
    "            best_pos = None\n",
    "            for job in state.unassigned:\n",
    "                costs = []\n",
    "                for idx in range(len(state.schedule) + 1):\n",
    "                    state.insert(job, idx)\n",
    "                    costs.append(state.objective())\n",
    "                    state.remove(job)\n",
    "                if len(costs) >= 2:\n",
    "                    sorted_costs = sorted(costs)\n",
    "                    regret = sorted_costs[1] - sorted_costs[0]\n",
    "                else:\n",
    "                    regret = 0\n",
    "                if regret > best_regret:\n",
    "                    best_regret = regret\n",
    "                    best_job = job\n",
    "                    best_pos = costs.index(min(costs))\n",
    "            state.insert(best_job, best_pos)\n",
    "            state.unassigned.remove(best_job)\n",
    "\n",
    "    elif strategy == \"randomized_greedy\":\n",
    "        # Randomized greedy: for each job, pick one of the k-best positions at random\n",
    "        k = min(3, len(state.schedule) + 1)\n",
    "        for job in state.unassigned[:]:\n",
    "            costs = []\n",
    "            for idx in range(len(state.schedule) + 1):\n",
    "                state.insert(job, idx)\n",
    "                costs.append((state.objective(), idx))\n",
    "                state.remove(job)\n",
    "            costs.sort()\n",
    "            chosen = rng.choice(costs[:k])\n",
    "            state.insert(job, chosen[1])\n",
    "            state.unassigned.remove(job)\n",
    "\n",
    "    elif strategy == \"block_insertion\":\n",
    "        # Insert all unassigned jobs as a block at the best position\n",
    "        block = state.unassigned[:]\n",
    "        best_pos = None\n",
    "        best_makespan = float('inf')\n",
    "        for idx in range(len(state.schedule) + 1):\n",
    "            for offset, job in enumerate(block):\n",
    "                state.insert(job, idx + offset)\n",
    "            ms = state.objective()\n",
    "            if ms < best_makespan:\n",
    "                best_makespan = ms\n",
    "                best_pos = idx\n",
    "            for job in block:\n",
    "                state.remove(job)\n",
    "        for offset, job in enumerate(block):\n",
    "            state.insert(job, best_pos + offset)\n",
    "            state.unassigned.remove(job)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing instance: j20_m5_01 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1286, Gap: 0.63%, Time: 0.38s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1286, Gap: 0.63%, Time: 1.12s\n",
      "\n",
      "Processing instance: j20_m5_02 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1360, Gap: 0.07%, Time: 0.38s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1365, Gap: 0.44%, Time: 1.12s\n",
      "\n",
      "Processing instance: j20_m5_03 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1087, Gap: 0.56%, Time: 0.39s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1100, Gap: 1.76%, Time: 1.27s\n",
      "\n",
      "Processing instance: j20_m5_04 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1299, Gap: 0.46%, Time: 0.37s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1306, Gap: 1.01%, Time: 1.10s\n",
      "\n",
      "Processing instance: j20_m5_05 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1235, Gap: -0.08%, Time: 0.37s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1250, Gap: 1.13%, Time: 1.10s\n",
      "\n",
      "Processing instance: j20_m5_06 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1207, Gap: 1.00%, Time: 0.36s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1210, Gap: 1.26%, Time: 1.11s\n",
      "\n",
      "Processing instance: j20_m5_07 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1251, Gap: 0.97%, Time: 0.42s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1248, Gap: 0.73%, Time: 1.26s\n",
      "\n",
      "Processing instance: j20_m5_08 (Type: j20_m5)\n",
      "  Running Baseline...\n",
      "    Objective: 1206, Gap: 0.00%, Time: 0.39s\n",
      "  Running CHATGPT...\n",
      "    Objective: 1206, Gap: 0.00%, Time: 1.17s\n",
      "\n",
      "Processing instance: j20_m5_09 (Type: j20_m5)\n",
      "  Running Baseline...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 244\u001b[0m\n\u001b[1;32m    241\u001b[0m     data_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/j100_m10/j100_m10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Run the benchmark\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproaches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Visualize and analyze the results\u001b[39;00m\n\u001b[1;32m    247\u001b[0m results_df \u001b[38;5;241m=\u001b[39m visualize_results(results)\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mrun_benchmark\u001b[0;34m(data_files, approaches, seed, iters)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Run ALNS\u001b[39;00m\n\u001b[1;32m     76\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 77\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43malns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m runtime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Record results\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/alns/ALNS.py:216\u001b[0m, in \u001b[0;36mALNS.iterate\u001b[0;34m(self, initial_solution, op_select, accept, stop, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m stats\u001b[38;5;241m.\u001b[39mcollect_runtime(time\u001b[38;5;241m.\u001b[39mperf_counter())\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng, best, curr):\n\u001b[0;32m--> 216\u001b[0m     d_idx, r_idx \u001b[38;5;241m=\u001b[39m \u001b[43mop_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     d_name, d_operator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestroy_operators[d_idx]\n\u001b[1;32m    219\u001b[0m     r_name, r_operator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepair_operators[r_idx]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/alns/select/AlphaUCB.py:109\u001b[0m, in \u001b[0;36mAlphaUCB.__call__\u001b[0;34m(self, rng, best, curr)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rng, best, curr):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    Returns the (destroy, repair) operator pair that maximises the average\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    reward and exploration bonus.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(np\u001b[38;5;241m.\u001b[39munravel_index(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop_coupling\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/alns/select/AlphaUCB.py:141\u001b[0m, in \u001b[0;36mAlphaUCB._values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter\n\u001b[1;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_avg_rewards\n\u001b[0;32m--> 141\u001b[0m explore_bonus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((a \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    143\u001b[0m values \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m+\u001b[39m explore_bonus\n\u001b[1;32m    144\u001b[0m values[\u001b[38;5;241m~\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_coupling] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# avoid selecting disallowed pairs\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_benchmark(data_files, approaches, seed=SEED, iters=8000):\n",
    "    \"\"\"\n",
    "    Benchmark different ALNS approaches on multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        data_files: List of paths to problem instance files\n",
    "        approaches: Dictionary mapping approach names to lists of (destroy_ops, repair_ops)\n",
    "        seed: Random seed for reproducibility\n",
    "        iters: Number of iterations for each ALNS run\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all benchmark results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'instance_names': [],\n",
    "        'instance_sizes': [],\n",
    "        'problem_types': [],\n",
    "        'best_known_values': [],\n",
    "    }\n",
    "    \n",
    "    # Initialize results dictionary for each approach\n",
    "    for approach_name in approaches:\n",
    "        results[f'{approach_name}_objectives'] = []\n",
    "        results[f'{approach_name}_gaps'] = []\n",
    "        results[f'{approach_name}_times'] = []\n",
    "        results[f'{approach_name}_results'] = []  # Store the ALNS result objects\n",
    "    \n",
    "    for data_file in data_files:\n",
    "        # Extract instance name from file path\n",
    "        instance_name = data_file.split('/')[-1].split('.')[0]\n",
    "        problem_type = data_file.split('/')[-2]\n",
    "        print(f\"\\nProcessing instance: {instance_name} (Type: {problem_type})\")\n",
    "        \n",
    "        # Load data\n",
    "        data = Data.from_file(data_file)\n",
    "        global DATA  # Use global DATA variable for the operators\n",
    "        DATA = data\n",
    "        \n",
    "        results['instance_names'].append(instance_name)\n",
    "        results['problem_types'].append(problem_type)\n",
    "        results['instance_sizes'].append(f\"{data.n_jobs}x{data.n_machines}\")\n",
    "        results['best_known_values'].append(data.bkv)\n",
    "        \n",
    "        # Create initial solution using NEH\n",
    "        init = NEH(data.processing_times)\n",
    "        \n",
    "        # Run each approach\n",
    "        for approach_name, (destroy_ops, repair_ops) in approaches.items():\n",
    "            print(f\"  Running {approach_name}...\")\n",
    "            \n",
    "            # Setup ALNS\n",
    "            alns = ALNS(rnd.default_rng(seed))\n",
    "            \n",
    "            # Add destroy operators\n",
    "            for destroy_op in destroy_ops:\n",
    "                alns.add_destroy_operator(destroy_op)\n",
    "            \n",
    "            # Add repair operators\n",
    "            for repair_op in repair_ops:\n",
    "                alns.add_repair_operator(repair_op)\n",
    "            \n",
    "            # Configure ALNS parameters\n",
    "            select = AlphaUCB(\n",
    "                scores=[5, 2, 1, 0.5],\n",
    "                alpha=0.05,\n",
    "                num_destroy=len(alns.destroy_operators),\n",
    "                num_repair=len(alns.repair_operators),\n",
    "            )\n",
    "            accept = SimulatedAnnealing.autofit(init.objective(), 0.05, 0.50, iters)\n",
    "            stop = MaxIterations(iters)\n",
    "            \n",
    "            # Add time tracking\n",
    "            time_stop = MaxRuntime(3600)  # 1 hour max runtime\n",
    "            \n",
    "            # Run ALNS\n",
    "            start_time = time.time()\n",
    "            result = alns.iterate(deepcopy(init), select, accept, stop)\n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            # Record results\n",
    "            objective = result.best_state.objective()\n",
    "            gap = 100 * (objective - data.bkv) / data.bkv\n",
    "            \n",
    "            results[f'{approach_name}_objectives'].append(objective)\n",
    "            results[f'{approach_name}_gaps'].append(gap)\n",
    "            results[f'{approach_name}_times'].append(runtime)\n",
    "            results[f'{approach_name}_results'].append(result)  # Store the ALNS result object\n",
    "            \n",
    "            print(f\"    Objective: {objective}, Gap: {gap:.2f}%, Time: {runtime:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"\n",
    "    Create visualizations to compare the approaches.\n",
    "    \"\"\"\n",
    "    approach_names = [name.split('_')[0] for name in results.keys() \n",
    "                     if name.endswith('_objectives')]\n",
    "    \n",
    "    # Create a DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'Instance': results['instance_names'],\n",
    "        'Problem_Type': results['problem_types'],\n",
    "        'Size': results['instance_sizes'],\n",
    "        'BKV': results['best_known_values']\n",
    "    })\n",
    "    \n",
    "    for approach in approach_names:\n",
    "        df[f'{approach}_Obj'] = results[f'{approach}_objectives']\n",
    "        df[f'{approach}_Gap'] = results[f'{approach}_gaps']\n",
    "        df[f'{approach}_Time'] = results[f'{approach}_times']\n",
    "    \n",
    "    # 1. Problem type average gap comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Calculate average gap by problem type\n",
    "    problem_type_avg = df.groupby('Problem_Type').apply(\n",
    "        lambda x: pd.Series({\n",
    "            f\"{approach}_AvgGap\": x[f\"{approach}_Gap\"].mean() \n",
    "            for approach in approach_names\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    problem_types = problem_type_avg['Problem_Type']\n",
    "    x = np.arange(len(problem_types))\n",
    "    width = 0.8 / len(approach_names)\n",
    "    \n",
    "    for i, approach in enumerate(approach_names):\n",
    "        plt.bar(x + i*width - 0.4 + width/2, problem_type_avg[f'{approach}_AvgGap'], \n",
    "                width=width, label=approach)\n",
    "    \n",
    "    plt.xlabel('Problem Type')\n",
    "    plt.ylabel('Average Gap to BKV (%)')\n",
    "    plt.title('Average Performance Gap by Problem Type')\n",
    "    plt.xticks(x, problem_types, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Problem type average runtime comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Calculate average runtime by problem type\n",
    "    problem_type_avg_time = df.groupby('Problem_Type').apply(\n",
    "        lambda x: pd.Series({\n",
    "            f\"{approach}_AvgTime\": x[f\"{approach}_Time\"].mean() \n",
    "            for approach in approach_names\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    for i, approach in enumerate(approach_names):\n",
    "        plt.bar(x + i*width - 0.4 + width/2, problem_type_avg_time[f'{approach}_AvgTime'], \n",
    "                width=width, label=approach)\n",
    "    \n",
    "    plt.xlabel('Problem Type')\n",
    "    plt.ylabel('Average Runtime (seconds)')\n",
    "    plt.title('Average Runtime by Problem Type')\n",
    "    plt.xticks(x, problem_types, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Summary table by problem type\n",
    "    summary = pd.DataFrame({\n",
    "        'Problem_Type': problem_types,\n",
    "    })\n",
    "    \n",
    "    for approach in approach_names:\n",
    "        summary[f'{approach}_AvgGap'] = problem_type_avg[f'{approach}_AvgGap']\n",
    "    \n",
    "    display(summary)\n",
    "    \n",
    "    # 4. Overall summary\n",
    "    overall_summary = pd.DataFrame({\n",
    "        'Approach': approach_names,\n",
    "        'Avg Gap (%)': [df[f'{approach}_Gap'].mean() for approach in approach_names],\n",
    "        'Best Gap (%)': [df[f'{approach}_Gap'].min() for approach in approach_names],\n",
    "        'Worst Gap (%)': [df[f'{approach}_Gap'].max() for approach in approach_names],\n",
    "        'Avg Time (s)': [df[f'{approach}_Time'].mean() for approach in approach_names],\n",
    "    })\n",
    "    \n",
    "    display(overall_summary)\n",
    "    \n",
    "    # 5. Detailed results table\n",
    "    display(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Define the approaches to compare\n",
    "    approaches = {\n",
    "        'Baseline': (\n",
    "            [random_removal, adjacent_removal],  # destroy operators\n",
    "            [greedy_repair]    # repair operators\n",
    "        ),\n",
    "        'CHATGPT': (\n",
    "            [random_removal, adjacent_removal],  # destroy operators\n",
    "            [llm_repair]    # repair operators\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # List all relevant Taillard instances - 10 instances for each problem type up to j100_m10\n",
    "    data_files = []\n",
    "    \n",
    "    # j20_m5 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j20_m5/j20_m5_{i:02d}.txt\")\n",
    "        \n",
    "    # j20_m10 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j20_m10/j20_m10_{i:02d}.txt\")\n",
    "        \n",
    "    # j20_m20 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j20_m20/j20_m20_{i:02d}.txt\")\n",
    "        \n",
    "    # j50_m5 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j50_m5/j50_m5_{i:02d}.txt\")\n",
    "        \n",
    "    # j50_m10 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j50_m10/j50_m10_{i:02d}.txt\")\n",
    "        \n",
    "    # j50_m20 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j50_m20/j50_m20_{i:02d}.txt\")\n",
    "        \n",
    "    # j100_m5 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j100_m5/j100_m5_{i:02d}.txt\")\n",
    "        \n",
    "    # j100_m10 (10 instances)\n",
    "    for i in range(1, 11):\n",
    "        data_files.append(f\"data/j100_m10/j100_m10_{i:02d}.txt\")\n",
    "    \n",
    "    # Run the benchmark\n",
    "    results = run_benchmark(data_files, approaches, seed=SEED, iters=600)\n",
    "    \n",
    "    # Visualize and analyze the results\n",
    "    results_df = visualize_results(results)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('alns_operator_selection_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
